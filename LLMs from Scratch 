{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMtDzMlgiov8rPPtrVE6DFQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MOHAMEDSHOKR/LLMs/blob/main/LLMs%20from%20Scratch%20\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "import re\n",
        "url = (\"https://raw.githubusercontent.com/rasbt/\"\n",
        "\"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
        "\"the-verdict.txt\")\n",
        "file_path = \"the-verdict.txt\"\n",
        "urllib.request.urlretrieve(url, file_path)\n",
        "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "     raw_text = f.read()\n",
        "# new_text = \"\"\" It's the last he painted, you know,\"\n",
        "# Mrs. Gisburn said with pardonable pride.\"\"\"\n",
        "print(\"Total number of character:\", len(raw_text))\n",
        "print(raw_text[:99])\n",
        "\n",
        "preprocced = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "preprocced = [item.strip() for item in preprocced if item.strip()]\n",
        "\n",
        "print(len(preprocced))\n",
        "print(preprocced[:30])\n",
        "\n",
        "all_words = sorted(set(preprocced))\n",
        "\n",
        "vocab_size = len(all_words)\n",
        "print(vocab_size)\n",
        "\n",
        "# now we will impelment endcod method to converrt input text into tokens\n",
        "#convert Input Text in to tokens then into vocabualry(added for every unique token, token ID)\n",
        "\n",
        "vocab = {token:integer for integer, token in enumerate(all_words)}\n",
        "for i, item in enumerate(vocab.items()):\n",
        "     print(item)\n",
        "\n",
        "     if i >= 50:\n",
        "\n",
        "      break\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8JehRD61KhEH",
        "outputId": "6dac28d0-8dc6-4cfe-a78e-06747348f354"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of character: 20479\n",
            "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n",
            "7\n",
            "['Hello', ',', 'do', 'you', 'like', 'tea', '?']\n",
            "7\n",
            "(',', 0)\n",
            "('?', 1)\n",
            "('Hello', 2)\n",
            "('do', 3)\n",
            "('like', 4)\n",
            "('tea', 5)\n",
            "('you', 6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-bWCwQ6O-Q3",
        "outputId": "70af96dc-4af6-4167-ad47-b40e3a3148fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'!': 0, '\"': 1, \"'\": 2, '(': 3, ')': 4, ',': 5, '--': 6, '.': 7, ':': 8, ';': 9, '?': 10, 'A': 11, 'Ah': 12, 'Among': 13, 'And': 14, 'Are': 15, 'Arrt': 16, 'As': 17, 'At': 18, 'Be': 19, 'Begin': 20, 'Burlington': 21, 'But': 22, 'By': 23, 'Carlo': 24, 'Chicago': 25, 'Claude': 26, 'Come': 27, 'Croft': 28, 'Destroyed': 29, 'Devonshire': 30, 'Don': 31, 'Dubarry': 32, 'Emperors': 33, 'Florence': 34, 'For': 35, 'Gallery': 36, 'Gideon': 37, 'Gisburn': 38, 'Gisburns': 39, 'Grafton': 40, 'Greek': 41, 'Grindle': 42, 'Grindles': 43, 'HAD': 44, 'Had': 45, 'Hang': 46, 'Has': 47, 'He': 48, 'Her': 49, 'Hermia': 50, 'His': 51, 'How': 52, 'I': 53, 'If': 54, 'In': 55, 'It': 56, 'Jack': 57, 'Jove': 58, 'Just': 59, 'Lord': 60, 'Made': 61, 'Miss': 62, 'Money': 63, 'Monte': 64, 'Moon-dancers': 65, 'Mr': 66, 'Mrs': 67, 'My': 68, 'Never': 69, 'No': 70, 'Now': 71, 'Nutley': 72, 'Of': 73, 'Oh': 74, 'On': 75, 'Once': 76, 'Only': 77, 'Or': 78, 'Perhaps': 79, 'Poor': 80, 'Professional': 81, 'Renaissance': 82, 'Rickham': 83, 'Riviera': 84, 'Rome': 85, 'Russian': 86, 'Sevres': 87, 'She': 88, 'Stroud': 89, 'Strouds': 90, 'Suddenly': 91, 'That': 92, 'The': 93, 'Then': 94, 'There': 95, 'They': 96, 'This': 97, 'Those': 98, 'Though': 99, 'Thwing': 100, 'Thwings': 101, 'To': 102, 'Usually': 103, 'Venetian': 104, 'Victor': 105, 'Was': 106, 'We': 107, 'Well': 108, 'What': 109, 'When': 110, 'Why': 111, 'Yes': 112, 'You': 113, '_': 114, 'a': 115, 'abdication': 116, 'able': 117, 'about': 118, 'above': 119, 'abruptly': 120, 'absolute': 121, 'absorbed': 122, 'absurdity': 123, 'academic': 124, 'accuse': 125, 'accustomed': 126, 'across': 127, 'activity': 128, 'add': 129, 'added': 130, 'admirers': 131, 'adopted': 132, 'adulation': 133, 'advance': 134, 'aesthetic': 135, 'affect': 136, 'afraid': 137, 'after': 138, 'afterward': 139, 'again': 140, 'ago': 141, 'ah': 142, 'air': 143, 'alive': 144, 'all': 145, 'almost': 146, 'alone': 147, 'along': 148, 'always': 149, 'am': 150, 'amazement': 151, 'amid': 152, 'among': 153, 'amplest': 154, 'amusing': 155, 'an': 156, 'and': 157, 'another': 158, 'answer': 159, 'answered': 160, 'any': 161, 'anything': 162, 'anywhere': 163, 'apparent': 164, 'apparently': 165, 'appearance': 166, 'appeared': 167, 'appointed': 168, 'are': 169, 'arm': 170, 'arm-chair': 171, 'arm-chairs': 172, 'arms': 173, 'art': 174, 'articles': 175, 'artist': 176, 'as': 177, 'aside': 178, 'asked': 179, 'at': 180, 'atmosphere': 181, 'atom': 182, 'attack': 183, 'attention': 184, 'attitude': 185, 'audacities': 186, 'away': 187, 'awful': 188, 'axioms': 189, 'azaleas': 190, 'back': 191, 'background': 192, 'balance': 193, 'balancing': 194, 'balustraded': 195, 'basking': 196, 'bath-rooms': 197, 'be': 198, 'beaming': 199, 'bean-stalk': 200, 'bear': 201, 'beard': 202, 'beauty': 203, 'became': 204, 'because': 205, 'becoming': 206, 'bed': 207, 'been': 208, 'before': 209, 'began': 210, 'begun': 211, 'behind': 212, 'being': 213, 'believed': 214, 'beneath': 215, 'bespoke': 216, 'better': 217, 'between': 218, 'big': 219, 'bits': 220, 'bitterness': 221, 'blocked': 222, 'born': 223, 'borne': 224, 'boudoir': 225, 'bravura': 226, 'break': 227, 'breaking': 228, 'breathing': 229, 'bric-a-brac': 230, 'briefly': 231, 'brings': 232, 'bronzes': 233, 'brought': 234, 'brown': 235, 'brush': 236, 'bull': 237, 'business': 238, 'but': 239, 'buying': 240, 'by': 241, 'called': 242, 'came': 243, 'can': 244, 'canvas': 245, 'canvases': 246, 'cards': 247, 'care': 248, 'career': 249, 'caught': 250, 'central': 251, 'chair': 252, 'chap': 253, 'characteristic': 254, 'charming': 255, 'cheap': 256, 'check': 257, 'cheeks': 258, 'chest': 259, 'chimney-piece': 260, 'chucked': 261, 'cigar': 262, 'cigarette': 263, 'cigars': 264, 'circulation': 265, 'circumstance': 266, 'circus-clown': 267, 'claimed': 268, 'clasping': 269, 'clear': 270, 'cleverer': 271, 'close': 272, 'clue': 273, 'coat': 274, 'collapsed': 275, 'colour': 276, 'come': 277, 'comfortable': 278, 'coming': 279, 'companion': 280, 'compared': 281, 'complex': 282, 'confident': 283, 'congesting': 284, 'conjugal': 285, 'constraint': 286, 'consummate': 287, 'contended': 288, 'continued': 289, 'corner': 290, 'corrected': 291, 'could': 292, 'couldn': 293, 'count': 294, 'countenance': 295, 'couple': 296, 'course': 297, 'covered': 298, 'craft': 299, 'cried': 300, 'crossed': 301, 'crowned': 302, 'crumbled': 303, 'cry': 304, 'cured': 305, 'curiosity': 306, 'curious': 307, 'current': 308, 'curtains': 309, 'd': 310, 'dabble': 311, 'damask': 312, 'dark': 313, 'dashed': 314, 'day': 315, 'days': 316, 'dead': 317, 'deadening': 318, 'dear': 319, 'deep': 320, 'deerhound': 321, 'degree': 322, 'delicate': 323, 'demand': 324, 'denied': 325, 'deploring': 326, 'deprecating': 327, 'deprecatingly': 328, 'desire': 329, 'destroyed': 330, 'destruction': 331, 'desultory': 332, 'detail': 333, 'diagnosis': 334, 'did': 335, 'didn': 336, 'died': 337, 'dim': 338, 'dimmest': 339, 'dingy': 340, 'dining-room': 341, 'disarming': 342, 'discovery': 343, 'discrimination': 344, 'discussion': 345, 'disdain': 346, 'disdained': 347, 'disease': 348, 'disguised': 349, 'display': 350, 'dissatisfied': 351, 'distinguished': 352, 'distract': 353, 'divert': 354, 'do': 355, 'doesn': 356, 'doing': 357, 'domestic': 358, 'don': 359, 'done': 360, 'donkey': 361, 'down': 362, 'dozen': 363, 'dragged': 364, 'drawing-room': 365, 'drawing-rooms': 366, 'drawn': 367, 'dress-closets': 368, 'drew': 369, 'dropped': 370, 'each': 371, 'earth': 372, 'ease': 373, 'easel': 374, 'easy': 375, 'echoed': 376, 'economy': 377, 'effect': 378, 'effects': 379, 'efforts': 380, 'egregious': 381, 'eighteenth-century': 382, 'elbow': 383, 'elegant': 384, 'else': 385, 'embarrassed': 386, 'enabled': 387, 'end': 388, 'endless': 389, 'enjoy': 390, 'enlightenment': 391, 'enough': 392, 'ensuing': 393, 'equally': 394, 'equanimity': 395, 'escape': 396, 'established': 397, 'etching': 398, 'even': 399, 'event': 400, 'ever': 401, 'everlasting': 402, 'every': 403, 'exasperated': 404, 'except': 405, 'excuse': 406, 'excusing': 407, 'existed': 408, 'expected': 409, 'exquisite': 410, 'exquisitely': 411, 'extenuation': 412, 'exterminating': 413, 'extracting': 414, 'eye': 415, 'eyebrows': 416, 'eyes': 417, 'face': 418, 'faces': 419, 'fact': 420, 'faded': 421, 'failed': 422, 'failure': 423, 'fair': 424, 'faith': 425, 'false': 426, 'familiar': 427, 'famille-verte': 428, 'fancy': 429, 'fashionable': 430, 'fate': 431, 'feather': 432, 'feet': 433, 'fell': 434, 'fellow': 435, 'felt': 436, 'few': 437, 'fewer': 438, 'finality': 439, 'find': 440, 'fingers': 441, 'first': 442, 'fit': 443, 'fitting': 444, 'five': 445, 'flash': 446, 'flashed': 447, 'florid': 448, 'flowers': 449, 'fluently': 450, 'flung': 451, 'follow': 452, 'followed': 453, 'fond': 454, 'footstep': 455, 'for': 456, 'forced': 457, 'forcing': 458, 'forehead': 459, 'foreign': 460, 'foreseen': 461, 'forgive': 462, 'forgotten': 463, 'form': 464, 'formed': 465, 'forming': 466, 'forward': 467, 'fostered': 468, 'found': 469, 'foundations': 470, 'fragment': 471, 'fragments': 472, 'frame': 473, 'frames': 474, 'frequently': 475, 'friend': 476, 'from': 477, 'full': 478, 'fullest': 479, 'furiously': 480, 'furrowed': 481, 'garlanded': 482, 'garlands': 483, 'gave': 484, 'genial': 485, 'genius': 486, 'gesture': 487, 'get': 488, 'getting': 489, 'give': 490, 'given': 491, 'glad': 492, 'glanced': 493, 'glimpse': 494, 'gloried': 495, 'glory': 496, 'go': 497, 'going': 498, 'gone': 499, 'good': 500, 'good-breeding': 501, 'good-humoured': 502, 'got': 503, 'grace': 504, 'gradually': 505, 'gray': 506, 'grayish': 507, 'great': 508, 'greatest': 509, 'greatness': 510, 'grew': 511, 'groping': 512, 'growing': 513, 'had': 514, 'hadn': 515, 'hair': 516, 'half': 517, 'half-light': 518, 'half-mechanically': 519, 'hall': 520, 'hand': 521, 'hands': 522, 'handsome': 523, 'hanging': 524, 'happen': 525, 'happened': 526, 'hard': 527, 'hardly': 528, 'has': 529, 'have': 530, 'haven': 531, 'having': 532, 'he': 533, 'head': 534, 'hear': 535, 'heard': 536, 'heart': 537, 'height': 538, 'her': 539, 'here': 540, 'hermit': 541, 'herself': 542, 'hesitations': 543, 'hide': 544, 'high': 545, 'him': 546, 'himself': 547, 'hint': 548, 'his': 549, 'history': 550, 'holding': 551, 'home': 552, 'honour': 553, 'hooded': 554, 'hostess': 555, 'hot-house': 556, 'hour': 557, 'hours': 558, 'house': 559, 'how': 560, 'hung': 561, 'husband': 562, 'idea': 563, 'idle': 564, 'idling': 565, 'if': 566, 'immediately': 567, 'in': 568, 'incense': 569, 'indifferent': 570, 'inevitable': 571, 'inevitably': 572, 'inflexible': 573, 'insensible': 574, 'insignificant': 575, 'instinctively': 576, 'instructive': 577, 'interesting': 578, 'into': 579, 'ironic': 580, 'irony': 581, 'irrelevance': 582, 'irrevocable': 583, 'is': 584, 'it': 585, 'its': 586, 'itself': 587, 'jardiniere': 588, 'jealousy': 589, 'just': 590, 'keep': 591, 'kept': 592, 'kind': 593, 'knees': 594, 'knew': 595, 'know': 596, 'known': 597, 'laid': 598, 'lair': 599, 'landing': 600, 'language': 601, 'last': 602, 'late': 603, 'later': 604, 'latter': 605, 'laugh': 606, 'laughed': 607, 'lay': 608, 'leading': 609, 'lean': 610, 'learned': 611, 'least': 612, 'leathery': 613, 'leave': 614, 'led': 615, 'left': 616, 'leisure': 617, 'lends': 618, 'lent': 619, 'let': 620, 'lies': 621, 'life': 622, 'life-likeness': 623, 'lift': 624, 'lifted': 625, 'light': 626, 'lightly': 627, 'like': 628, 'liked': 629, 'line': 630, 'lines': 631, 'lingered': 632, 'lips': 633, 'lit': 634, 'little': 635, 'live': 636, 'll': 637, 'loathing': 638, 'long': 639, 'longed': 640, 'longer': 641, 'look': 642, 'looked': 643, 'looking': 644, 'lose': 645, 'loss': 646, 'lounging': 647, 'lovely': 648, 'lucky': 649, 'lump': 650, 'luncheon-table': 651, 'luxury': 652, 'lying': 653, 'made': 654, 'make': 655, 'man': 656, 'manage': 657, 'managed': 658, 'mantel-piece': 659, 'marble': 660, 'married': 661, 'may': 662, 'me': 663, 'meant': 664, 'mediocrity': 665, 'medium': 666, 'mentioned': 667, 'mere': 668, 'merely': 669, 'met': 670, 'might': 671, 'mighty': 672, 'millionaire': 673, 'mine': 674, 'minute': 675, 'minutes': 676, 'mirrors': 677, 'modest': 678, 'modesty': 679, 'moment': 680, 'money': 681, 'monumental': 682, 'mood': 683, 'morbidly': 684, 'more': 685, 'most': 686, 'mourn': 687, 'mourned': 688, 'moustache': 689, 'moved': 690, 'much': 691, 'muddling': 692, 'multiplied': 693, 'murmur': 694, 'muscles': 695, 'must': 696, 'my': 697, 'myself': 698, 'mysterious': 699, 'naive': 700, 'near': 701, 'nearly': 702, 'negatived': 703, 'nervous': 704, 'nervousness': 705, 'neutral': 706, 'never': 707, 'next': 708, 'no': 709, 'none': 710, 'not': 711, 'note': 712, 'nothing': 713, 'now': 714, 'nymphs': 715, 'oak': 716, 'obituary': 717, 'object': 718, 'objects': 719, 'occurred': 720, 'oddly': 721, 'of': 722, 'off': 723, 'often': 724, 'oh': 725, 'old': 726, 'on': 727, 'once': 728, 'one': 729, 'ones': 730, 'only': 731, 'onto': 732, 'open': 733, 'or': 734, 'other': 735, 'our': 736, 'ourselves': 737, 'out': 738, 'outline': 739, 'oval': 740, 'over': 741, 'own': 742, 'packed': 743, 'paid': 744, 'paint': 745, 'painted': 746, 'painter': 747, 'painting': 748, 'pale': 749, 'paled': 750, 'palm-trees': 751, 'panel': 752, 'panelling': 753, 'pardonable': 754, 'pardoned': 755, 'part': 756, 'passages': 757, 'passing': 758, 'past': 759, 'pastels': 760, 'pathos': 761, 'patient': 762, 'people': 763, 'perceptible': 764, 'perfect': 765, 'persistence': 766, 'persuasively': 767, 'phrase': 768, 'picture': 769, 'pictures': 770, 'pines': 771, 'pink': 772, 'place': 773, 'placed': 774, 'plain': 775, 'platitudes': 776, 'pleased': 777, 'pockets': 778, 'point': 779, 'poised': 780, 'poor': 781, 'portrait': 782, 'posing': 783, 'possessed': 784, 'poverty': 785, 'predicted': 786, 'preliminary': 787, 'presenting': 788, 'prestidigitation': 789, 'pretty': 790, 'previous': 791, 'price': 792, 'pride': 793, 'princely': 794, 'prism': 795, 'problem': 796, 'proclaiming': 797, 'prodigious': 798, 'profusion': 799, 'protest': 800, 'prove': 801, 'public': 802, 'purblind': 803, 'purely': 804, 'pushed': 805, 'put': 806, 'qualities': 807, 'quality': 808, 'queerly': 809, 'question': 810, 'quickly': 811, 'quietly': 812, 'quite': 813, 'quote': 814, 'rain': 815, 'raised': 816, 'random': 817, 'rather': 818, 're': 819, 'real': 820, 'really': 821, 'reared': 822, 'reason': 823, 'reassurance': 824, 'recovering': 825, 'recreated': 826, 'reflected': 827, 'reflection': 828, 'regrets': 829, 'relatively': 830, 'remained': 831, 'remember': 832, 'reminded': 833, 'repeating': 834, 'represented': 835, 'reproduction': 836, 'resented': 837, 'resolve': 838, 'resources': 839, 'rest': 840, 'rich': 841, 'ridiculous': 842, 'robbed': 843, 'romantic': 844, 'room': 845, 'rose': 846, 'rs': 847, 'rule': 848, 'run': 849, 's': 850, 'said': 851, 'same': 852, 'satisfaction': 853, 'savour': 854, 'saw': 855, 'say': 856, 'saying': 857, 'says': 858, 'scorn': 859, 'scornful': 860, 'secret': 861, 'see': 862, 'seemed': 863, 'seen': 864, 'self-confident': 865, 'send': 866, 'sensation': 867, 'sensitive': 868, 'sent': 869, 'serious': 870, 'set': 871, 'sex': 872, 'shade': 873, 'shaking': 874, 'shall': 875, 'she': 876, 'shirked': 877, 'short': 878, 'should': 879, 'shoulder': 880, 'shoulders': 881, 'show': 882, 'showed': 883, 'showy': 884, 'shrug': 885, 'shrugged': 886, 'sight': 887, 'sign': 888, 'silent': 889, 'silver': 890, 'similar': 891, 'simpleton': 892, 'simplifications': 893, 'simply': 894, 'since': 895, 'single': 896, 'sitter': 897, 'sitters': 898, 'sketch': 899, 'skill': 900, 'slight': 901, 'slightly': 902, 'slowly': 903, 'small': 904, 'smile': 905, 'smiling': 906, 'sneer': 907, 'so': 908, 'solace': 909, 'some': 910, 'somebody': 911, 'something': 912, 'spacious': 913, 'spaniel': 914, 'speaking-tubes': 915, 'speculations': 916, 'spite': 917, 'splash': 918, 'square': 919, 'stairs': 920, 'stammer': 921, 'stand': 922, 'standing': 923, 'started': 924, 'stay': 925, 'still': 926, 'stocked': 927, 'stood': 928, 'stopped': 929, 'stopping': 930, 'straddling': 931, 'straight': 932, 'strain': 933, 'straining': 934, 'strange': 935, 'straw': 936, 'stream': 937, 'stroke': 938, 'strokes': 939, 'strolled': 940, 'strongest': 941, 'strongly': 942, 'struck': 943, 'studio': 944, 'stuff': 945, 'subject': 946, 'substantial': 947, 'suburban': 948, 'such': 949, 'suddenly': 950, 'suffered': 951, 'sugar': 952, 'suggested': 953, 'sunburn': 954, 'sunburnt': 955, 'sunlit': 956, 'superb': 957, 'sure': 958, 'surest': 959, 'surface': 960, 'surprise': 961, 'surprised': 962, 'surrounded': 963, 'suspected': 964, 'sweetly': 965, 'sweetness': 966, 'swelling': 967, 'swept': 968, 'swum': 969, 't': 970, 'table': 971, 'take': 972, 'taken': 973, 'talking': 974, 'tea': 975, 'tears': 976, 'technicalities': 977, 'technique': 978, 'tell': 979, 'tells': 980, 'tempting': 981, 'terra-cotta': 982, 'terrace': 983, 'terraces': 984, 'terribly': 985, 'than': 986, 'that': 987, 'the': 988, 'their': 989, 'them': 990, 'then': 991, 'there': 992, 'therefore': 993, 'they': 994, 'thin': 995, 'thing': 996, 'things': 997, 'think': 998, 'this': 999, 'thither': 1000, 'those': 1001, 'though': 1002, 'thought': 1003, 'three': 1004, 'threshold': 1005, 'threw': 1006, 'through': 1007, 'throwing': 1008, 'tie': 1009, 'till': 1010, 'time': 1011, 'timorously': 1012, 'tinge': 1013, 'tips': 1014, 'tired': 1015, 'to': 1016, 'told': 1017, 'tone': 1018, 'tones': 1019, 'too': 1020, 'took': 1021, 'tottering': 1022, 'touched': 1023, 'toward': 1024, 'trace': 1025, 'trade': 1026, 'transmute': 1027, 'traps': 1028, 'travelled': 1029, 'tribute': 1030, 'tributes': 1031, 'tricks': 1032, 'tried': 1033, 'trouser-presses': 1034, 'true': 1035, 'truth': 1036, 'turned': 1037, 'twenty': 1038, 'twenty-four': 1039, 'twice': 1040, 'twirling': 1041, 'unaccountable': 1042, 'uncertain': 1043, 'under': 1044, 'underlay': 1045, 'underneath': 1046, 'understand': 1047, 'unexpected': 1048, 'untouched': 1049, 'unusual': 1050, 'up': 1051, 'up-stream': 1052, 'upon': 1053, 'upset': 1054, 'upstairs': 1055, 'us': 1056, 'used': 1057, 'usual': 1058, 'value': 1059, 'varnishing': 1060, 'vases': 1061, 've': 1062, 'veins': 1063, 'velveteen': 1064, 'very': 1065, 'villa': 1066, 'vindicated': 1067, 'virtuosity': 1068, 'vista': 1069, 'vocation': 1070, 'voice': 1071, 'wall': 1072, 'wander': 1073, 'want': 1074, 'wanted': 1075, 'wants': 1076, 'was': 1077, 'wasn': 1078, 'watched': 1079, 'watching': 1080, 'water-colour': 1081, 'waves': 1082, 'way': 1083, 'weekly': 1084, 'weeks': 1085, 'welcome': 1086, 'went': 1087, 'were': 1088, 'what': 1089, 'when': 1090, 'whenever': 1091, 'where': 1092, 'which': 1093, 'while': 1094, 'white': 1095, 'white-panelled': 1096, 'who': 1097, 'whole': 1098, 'whom': 1099, 'why': 1100, 'wide': 1101, 'widow': 1102, 'wife': 1103, 'wild': 1104, 'wincing': 1105, 'window-curtains': 1106, 'wish': 1107, 'with': 1108, 'without': 1109, 'wits': 1110, 'woman': 1111, 'women': 1112, 'won': 1113, 'wonder': 1114, 'wondered': 1115, 'word': 1116, 'work': 1117, 'working': 1118, 'worth': 1119, 'would': 1120, 'wouldn': 1121, 'year': 1122, 'years': 1123, 'yellow': 1124, 'yet': 1125, 'you': 1126, 'younger': 1127, 'your': 1128, 'yourself': 1129}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Listing 2.3 Implementing a simple text tokenizer\n",
        "\"\"\" Stores the vocabulary as a class attribute for\n",
        "access in the encode and decode methods \"\"\"\n",
        "\n",
        "class SimpleTokenizerV1:\n",
        "\n",
        "    def __init__(self, vocab) :\n",
        "        self.str_into_int = vocab\n",
        "        self.int_into_str = {i:s for s, i in vocab.items()} #Creates an inverse vocabulary that maps token IDs back to the original text tokens\n",
        "\n",
        "    # process Input Text into text IDs\n",
        "    def encode(self, text):\n",
        "\n",
        "      preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
        "      preprocessed = [\n",
        "                item.strip() for item in preprocessed if item.strip()]\n",
        "      ids = [self.str_into_int[s] for s in preprocced]\n",
        "\n",
        "      return ids\n",
        "\n",
        "    #convert tokens IDs into text\n",
        "    def decode (self, ids):\n",
        "\n",
        "      text = \" \".join(self.int_into_str[i] for i in ids)\n",
        "\n",
        "      text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text) # remove spaces before the specified punctuation\n",
        "\n",
        "      return text\n",
        "\n",
        "new_text = \"\"\" It's the last he painted, you know,\"\n",
        "Mrs. Gisburn said with pardonable pride.\"\"\"\n",
        "\n",
        "tokenizer = SimpleTokenizerV1(vocab)\n",
        "\n",
        "\n",
        "ids = tokenizer.encode(new_text)\n",
        "\n",
        "print(ids)\n",
        "\n",
        "decoded_text = tokenizer.decode(ids)\n",
        "\n",
        "print(decoded_text)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxFprxIk-LtO",
        "outputId": "875c5644-0907-4921-df58-756891f5a202"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5, 1, 13, 15, 9, 7, 10, 2, 17, 8, 2, 0, 6, 3, 4, 14, 16, 11, 12, 3]\n",
            "It' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Listing 2.4 A simple text tokenizer that handles unknown words\n",
        "class SimpleTokenizerV2:\n",
        "\n",
        "  def __init__(self, vocab) :\n",
        "\n",
        "     self.str_into_int = vocab\n",
        "\n",
        "          # Add the <|unk|> token to the vocabulary if it's not present\n",
        "     if \"|<unk>|\" not in self.str_into_int:\n",
        "      self.str_into_int[\"|<unk>|\"] = len(self.str_into_int)\n",
        "\n",
        "     self.int_into_str = {i:s for s, i in vocab.items()}\n",
        "\n",
        "  def encode(self, text):\n",
        "\n",
        "    preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
        "    preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "\n",
        "    preprocessed = [item if item in self.str_into_int else \"<|unk|>\" for item in preprocessed]\n",
        "\n",
        "    ids = [self.str_into_int[s] for s in preprocced]\n",
        "    return ids\n",
        "\n",
        "  def decode (self, ids):\n",
        "\n",
        "    text = \" \".join(self.int_into_str[i] for i in ids)\n",
        "\n",
        "    text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "\n",
        "    return text\n",
        "\n",
        "text1 = \"Hello, do you like tea?\"\n",
        "text2 = \"In the sunlit terraces of the palace.\"\n",
        "text = \"|<endoftext>|\".join([text1, text2])\n",
        "\n",
        "print(text)\n",
        "\n",
        "tokenizer = SimpleTokenizerV2(vocab)\n",
        "print(tokenizer.encode(text))\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVGRkSahjvbB",
        "outputId": "769c2f34-c8b0-4adc-8feb-92dd884c0911"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, do you like tea?|<endoftext>|In the sunlit terraces of the palace.\n",
            "[2, 0, 3, 6, 4, 5, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" we will used advanced technice to decode and encode Input Text by using tokenizer from tiktoken library\n",
        "\n",
        "     tiktoken is library from Openai based on concept callled Byte pair Encoding (BPE) that used to train LLMs like GPT-2, GPT-3\n",
        "\"\"\"\n",
        "!pip install tiktoken\n",
        "from importlib.metadata import version\n",
        "import tiktoken\n",
        "\n",
        "print(version(\"tiktoken\"))\n",
        "\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "text = (\n",
        "\"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
        "\"of someunknownPlace.\"\n",
        ")\n",
        "\n",
        "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
        "print(integers)\n",
        "\n",
        "strings = tokenizer.decode(integers)\n",
        "print(strings)\n",
        "\n",
        "new_text = \"Akwirw ier\"\n",
        "integ = tokenizer.encode(new_text)\n",
        "print(integ)\n",
        "\n",
        "string = tokenizer.decode(integ)\n",
        "print(string)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQFXGVmaLqAn",
        "outputId": "bdaa4e34-bc2a-4d7b-a407-7144438a615a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.8.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.12.14)\n",
            "0.8.0\n",
            "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271, 13]\n",
            "Hello, do you like tea? <|endoftext|> In the sunlit terracesof someunknownPlace.\n",
            "[33901, 86, 343, 86, 220, 959]\n",
            "Akwirw ier\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import urllib.request\n",
        "import re\n",
        "import tiktoken\n",
        "url = (\"https://raw.githubusercontent.com/rasbt/\"\n",
        "\"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
        "\"the-verdict.txt\")\n",
        "file_path = \"the-verdict.txt\"\n",
        "urllib.request.urlretrieve(url, file_path)\n",
        "\n",
        "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "     raw_text = f.read()\n",
        "\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "enc = tokenizer.encode(raw_text)\n",
        "print(len(enc))\n",
        "en_sample = enc[50:]\n",
        "\n",
        "\"\"\"create two variables, x and y, where x contains the input\n",
        "tokens and y contains the targets, which are the inputs shifted by 1:\"\"\"\n",
        "\n",
        "context_size = 10\n",
        "x = en_sample[:context_size] # x contains the input tokens\n",
        "y = en_sample[1:context_size+1] # contains the targets, which are the inputs shifted by 1\n",
        "\n",
        "print(f\"x: {x}\")\n",
        "print(f\"y:        {y}\")\n",
        "\n",
        "# this is looping in encoding method\n",
        "for i in range(1, context_size+1):\n",
        "\n",
        "     context = en_sample[:i]\n",
        "     desired  = en_sample[i]\n",
        "\n",
        "     print(context, \"------->\", desired)\n",
        "\n",
        "# this is looping in decoding method\n",
        "for i in range(1, context_size+1):\n",
        "\n",
        "     context = en_sample[:i]\n",
        "     desired  = en_sample[i]\n",
        "\n",
        "     print(tokenizer.decode(context), \"------->\", tokenizer.decode([desired]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2ORPIGfFQki",
        "outputId": "63335466-a217-4567-db61-6da68bc8b95d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5145\n",
            "x: [290, 4920, 2241, 287, 257, 4489, 64, 319, 262, 34686]\n",
            "y:        [4920, 2241, 287, 257, 4489, 64, 319, 262, 34686, 41976]\n",
            "[290] -------> 4920\n",
            "[290, 4920] -------> 2241\n",
            "[290, 4920, 2241] -------> 287\n",
            "[290, 4920, 2241, 287] -------> 257\n",
            "[290, 4920, 2241, 287, 257] -------> 4489\n",
            "[290, 4920, 2241, 287, 257, 4489] -------> 64\n",
            "[290, 4920, 2241, 287, 257, 4489, 64] -------> 319\n",
            "[290, 4920, 2241, 287, 257, 4489, 64, 319] -------> 262\n",
            "[290, 4920, 2241, 287, 257, 4489, 64, 319, 262] -------> 34686\n",
            "[290, 4920, 2241, 287, 257, 4489, 64, 319, 262, 34686] -------> 41976\n",
            " and ------->  established\n",
            " and established ------->  himself\n",
            " and established himself ------->  in\n",
            " and established himself in ------->  a\n",
            " and established himself in a ------->  vill\n",
            " and established himself in a vill -------> a\n",
            " and established himself in a villa ------->  on\n",
            " and established himself in a villa on ------->  the\n",
            " and established himself in a villa on the ------->  Riv\n",
            " and established himself in a villa on the Riv -------> iera\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Listing 2.5 A dataset for batched inputs and targets\n",
        "import urllib.request\n",
        "import re\n",
        "import tiktoken\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "url = (\"https://raw.githubusercontent.com/rasbt/\"\n",
        "\"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
        "\"the-verdict.txt\")\n",
        "file_path = \"the-verdict.txt\"\n",
        "urllib.request.urlretrieve(url, file_path)\n",
        "\n",
        "class GPTDatasetV1(Dataset):\n",
        "\n",
        "  def __init__(self, txt, tokenizer, max_length, stride):\n",
        "\n",
        "    self.input_ids = []\n",
        "    self.target_ids = []\n",
        "\n",
        "    token_ids = tokenizer.encode(txt) # tokenize the entire text\n",
        "\n",
        "    # Uses a sliding window to chunk the book into overlapping sequences of max_length\n",
        "    for i in range(0, len(token_ids) - max_length, stride):\n",
        "\n",
        "      input_chunk = token_ids[ i : i + max_length ]\n",
        "      target_chunk = token_ids[ i + 1 : i + max_length + 1 ]\n",
        "\n",
        "      self.input_ids.append(torch.tensor(input_chunk))\n",
        "      self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "\n",
        "  # Returns the total number of rows in the dataset\n",
        "  def __len__(self):\n",
        "\n",
        "     return len(self.input_ids)\n",
        "\n",
        "  # Returns a single row from the dataset\n",
        "  def __getitem__(self, idx):\n",
        "\n",
        "    return self.input_ids[idx], self.target_ids[idx]\n",
        "\n",
        "# Listing 2.6 A data loader to generate batches with input-with pairs\n",
        "\n",
        "def create_dataloader_v1(txt, batch_size=4, max_length=256, stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
        "\n",
        "   tokenizer = tiktoken.get_encoding(\"gpt2\") # Initialize the tokenizer\n",
        "\n",
        "   dataset = GPTDatasetV1(txt, tokenizer, max_length, stride) # create dataset\n",
        "   dataloader = DataLoader(\n",
        "       dataset,\n",
        "       batch_size= batch_size,\n",
        "       shuffle= shuffle,\n",
        "       drop_last= drop_last, # drop_last=True drops the last batch if it is shorter than the specified batch_size to prevent loss spikes during training\n",
        "       num_workers= num_workers # The number of CPU processes to use for preprocessing\n",
        "       )\n",
        "\n",
        "   return dataloader\n",
        "\n",
        "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "     raw_text = f.read()\n",
        "\n",
        "\"\"\" The stride setting dictates the number of positions the inputs shift across batches, emulating a sliding window approach \"\"\"\n",
        "\n",
        "dataloader = create_dataloader_v1(raw_text, batch_size=1, max_length=8, stride=2,  shuffle=False)\n",
        "\n",
        "\n",
        "print(\"-\" * 45)\n",
        "# Converts dataloader into a Python iterator to fetch the next entry via Python’s built-in next() function\n",
        "data_itr = iter(dataloader)\n",
        "first_batch = next(data_itr)\n",
        "print(first_batch, end=\"\\n\")\n",
        "secod_batch = next(data_itr)\n",
        "print(secod_batch)\n",
        "\n",
        "print(\"-\" * 45)\n",
        "#create another initiate from dataloader with batch_size=8, and stride=4\n",
        "dataloader = create_dataloader_v1( raw_text, batch_size=8, max_length=4, stride=4, shuffle=False)\n",
        "data_iter = iter(dataloader)\n",
        "inputs, targets = next(data_iter)\n",
        "print(\"Inputs:\\n\", inputs)\n",
        "print(\"\\nTargets:\\n\", targets)\n",
        "\n",
        "print(\"-\" * 45)\n",
        "\n",
        "\"\"\" Let’s see how the token ID to embedding vector conversion works with a hands-on\n",
        "example. Suppose we have the following four input tokens with IDs 2, 3, 5, and 1:\"\"\"\n",
        "input_ids = torch.tensor([2, 3, 5, 1])\n",
        "\n",
        "vocab_size = 6\n",
        "output_dim = 3\n",
        "\n",
        "torch.manual_seed(128)\n",
        "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
        "\n",
        "print(embedding_layer.weight)\n",
        "\n",
        "print(\"-\" * 45)\n",
        "\n",
        "print(embedding_layer(torch.tensor([3])))\n",
        "\n",
        "print(embedding_layer(input_ids))\n",
        "\n",
        "print(\"-\" * 45)\n",
        "\n",
        "\n",
        "vocab_size = 50257\n",
        "output_dim = 256\n",
        "\n",
        "token_embeddeing_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
        "\n",
        "max_length = 4\n",
        "\n",
        "dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=max_length, stride=max_length, shuffle=False)\n",
        "\n",
        "data_iter = iter(dataloader)\n",
        "inputs, targets = next(data_iter)\n",
        "\n",
        "print(\"Token IDs: \\n\", inputs)\n",
        "print(\"\\nInputs shape:\\n\", inputs.shape)\n",
        "\n",
        "\"\"\" using the embedding layer to embed these token IDs into 256-dimensional vectors:\n",
        "\n",
        "    For a GPT model’s absolute embedding approach, we just need to create another embedding layer\n",
        "    that has the same embedding dimension as the token_embedding_layer: \"\"\"\n",
        "\n",
        "token_embeddeing = token_embeddeing_layer(inputs)\n",
        "\n",
        "print(token_embeddeing.shape)\n",
        "\n",
        "print(\"-\" * 45)\n",
        "\n",
        "context_length  = max_length\n",
        "\n",
        "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
        "pos_embedding = pos_embedding_layer(torch.arange(context_length))\n",
        "\n",
        "print(pos_embedding.shape)\n",
        "\n",
        "print(\"-\" * 45)\n",
        "\"\"\" We can now add these directly to the token embeddings, where PyTorch will add\n",
        "the 4 × 256–dimensional pos_embeddings tensor to each 4 × 256–dimensional token\n",
        "embedding tensor in each of the eight batches: \"\"\"\n",
        "\n",
        "\n",
        "input_tokens = token_embeddeing + pos_embedding\n",
        "print(input_tokens.shape)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sm1AjYfsRGO_",
        "outputId": "cf96c045-cbd5-4ce3-82a7-97c022d3dc56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------------------\n",
            "[tensor([[  40,  367, 2885, 1464, 1807, 3619,  402,  271]]), tensor([[  367,  2885,  1464,  1807,  3619,   402,   271, 10899]])]\n",
            "[tensor([[ 2885,  1464,  1807,  3619,   402,   271, 10899,  2138]]), tensor([[ 1464,  1807,  3619,   402,   271, 10899,  2138,   257]])]\n",
            "---------------------------------------------\n",
            "Inputs:\n",
            " tensor([[   40,   367,  2885,  1464],\n",
            "        [ 1807,  3619,   402,   271],\n",
            "        [10899,  2138,   257,  7026],\n",
            "        [15632,   438,  2016,   257],\n",
            "        [  922,  5891,  1576,   438],\n",
            "        [  568,   340,   373,   645],\n",
            "        [ 1049,  5975,   284,   502],\n",
            "        [  284,  3285,   326,    11]])\n",
            "\n",
            "Targets:\n",
            " tensor([[  367,  2885,  1464,  1807],\n",
            "        [ 3619,   402,   271, 10899],\n",
            "        [ 2138,   257,  7026, 15632],\n",
            "        [  438,  2016,   257,   922],\n",
            "        [ 5891,  1576,   438,   568],\n",
            "        [  340,   373,   645,  1049],\n",
            "        [ 5975,   284,   502,   284],\n",
            "        [ 3285,   326,    11,   287]])\n",
            "---------------------------------------------\n",
            "Parameter containing:\n",
            "tensor([[ 1.0117, -0.2076, -0.6215],\n",
            "        [-1.2875, -0.1075,  0.0563],\n",
            "        [-0.3184,  3.1566,  0.4621],\n",
            "        [-1.6456,  1.1484, -1.5457],\n",
            "        [-0.5344,  0.1237,  0.2927],\n",
            "        [ 0.6586,  0.3008,  0.1725]], requires_grad=True)\n",
            "---------------------------------------------\n",
            "tensor([[-1.6456,  1.1484, -1.5457]], grad_fn=<EmbeddingBackward0>)\n",
            "tensor([[-0.3184,  3.1566,  0.4621],\n",
            "        [-1.6456,  1.1484, -1.5457],\n",
            "        [ 0.6586,  0.3008,  0.1725],\n",
            "        [-1.2875, -0.1075,  0.0563]], grad_fn=<EmbeddingBackward0>)\n",
            "---------------------------------------------\n",
            "Token IDs: \n",
            " tensor([[   40,   367,  2885,  1464],\n",
            "        [ 1807,  3619,   402,   271],\n",
            "        [10899,  2138,   257,  7026],\n",
            "        [15632,   438,  2016,   257],\n",
            "        [  922,  5891,  1576,   438],\n",
            "        [  568,   340,   373,   645],\n",
            "        [ 1049,  5975,   284,   502],\n",
            "        [  284,  3285,   326,    11]])\n",
            "\n",
            "Inputs shape:\n",
            " torch.Size([8, 4])\n",
            "torch.Size([8, 4, 256])\n",
            "---------------------------------------------\n",
            "torch.Size([4, 256])\n",
            "---------------------------------------------\n",
            "torch.Size([8, 4, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# chapter3 Craeting Context vector for evry element in Input sentances as below\n",
        "import torch\n",
        "inputs = torch.tensor(\n",
        "[[0.43, 0.15, 0.89], # Your (x^1)\n",
        "[0.55, 0.87, 0.66], # journey (x^2)\n",
        "[0.57, 0.85, 0.64], # starts (x^3)\n",
        "[0.22, 0.58, 0.33], # with (x^4)\n",
        "[0.77, 0.25, 0.10], # one (x^5)\n",
        "[0.05, 0.80, 0.55]] # step (x^6)\n",
        ")\n",
        "\n",
        "\"\"\" to calculate attention context vector Z we will going by theree steps as below\n",
        "\n",
        "     1- Compute attention Scores W\n",
        "     2- Compute attention weights\n",
        "     3- Compute context vector Z\n",
        "     \"\"\"\n",
        "\n",
        "# 1- Compute attention Scores W\n",
        "\n",
        "query = inputs[1]\n",
        "\n",
        "attn_scores_2 = torch.empty(inputs.shape[0]) # calculating attention scores by using dot products funcation |a||b|cos0\n",
        "\n",
        "for i, x_i in enumerate(inputs):\n",
        "\n",
        "  attn_scores_2[i] = torch.dot( x_i , query )\n",
        "\n",
        "print(attn_scores_2)\n",
        "\n",
        "# 2- Compute attention weights\n",
        "\n",
        "attn_weights_2_tmp = attn_scores_2 / attn_scores_2.sum() # by using normalization fun to compute atten weights\n",
        "\n",
        "print(\"Attention Weights: \", attn_weights_2_tmp)\n",
        "\n",
        "print(\"Sum: \", attn_weights_2_tmp.sum())\n",
        "\n",
        "\"\"\" But In practice, it’s more common and advisable to use the softmax function for normalization\n",
        "\n",
        "    The softmax function, also known as softargmax or normalized exponential function,\n",
        "\n",
        "    converts a vector of K real numbers into a probability distribution of K possible outcomes\"\"\"\n",
        "\n",
        "def softmax_naive()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RDnNe07ejteh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8b9e4c8-e29a-4813-edce-131007eb0886"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n",
            "Attention Weights:  tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
            "Sum:  tensor(1.0000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3.4.1 Computing the attention weights step by step\n",
        "\n",
        "\"\"\"\n",
        "We will implement the self-attention mechanism step by step by introducing the\n",
        "three trainable weight matrices Wq, Wk, and Wv. These three matrices are used to\n",
        "project the embedded input tokens, x(i), into query, key, and value vectors, respectively \"\"\"\n",
        "\n",
        "import torch\n",
        "\n",
        "inputs = torch.tensor(\n",
        "[[0.43, 0.15, 0.89], # Your (x^1)\n",
        "[0.55, 0.87, 0.66], # journey (x^2)\n",
        "[0.57, 0.85, 0.64], # starts (x^3)\n",
        "[0.22, 0.58, 0.33], # with (x^4)\n",
        "[0.77, 0.25, 0.10], # one (x^5)\n",
        "[0.05, 0.80, 0.55]] # step (x^6)\n",
        ")\n",
        "\n",
        "x_2 = inputs[1]\n",
        "d_in = inputs.shape[1]\n",
        "\n",
        "d_out = 2\n",
        "\n",
        "torch.manual_seed(123)\n",
        "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
        "W_key = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
        "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
        "\n",
        "\n",
        "query_2 = x_2 @ W_query\n",
        "key_2 = x_2 @ W_key\n",
        "value_2 = x_2 @ W_value\n",
        "print(query_2)\n",
        "\n",
        "# to obtain all keys and values via matrix multiplication\n",
        "\n",
        "query = inputs @ W_query\n",
        "keys = inputs @ W_key\n",
        "values = inputs @ W_value\n",
        "print(\"query.shape:\", query.shape)\n",
        "print(\"keys.shape:\", keys.shape)\n",
        "print(\"values.shape:\", values.shape)\n",
        "\n",
        "#let’s compute the attention score ω22:\n",
        "keys_2 = keys[1]\n",
        "attn_score_22 = query_2.dot(keys_2)\n",
        "print(attn_score_22)\n",
        "\n",
        "#we can generalize this computation to all attention scores via matrix multiplication\n",
        "\n",
        "attn_scores_2 = query_2 @ keys.T\n",
        "print(\"attn_scores_2\",attn_scores_2)\n",
        "\n",
        "# We compute the attention weights by scaling the attention scores and using the softmax function\n",
        "\n",
        "d_k = keys.shape[-1]\n",
        "attn_weights_2 = torch.softmax(attn_scores_2 / d_k**0.5, dim=-1)\n",
        "print(\"attn_weights_2\",attn_weights_2)\n",
        "\n",
        "# compute the context vector by combining all value vectors via the attention weights.\n",
        "\n",
        "context_vec_2 = attn_weights_2 @ values\n",
        "print(\"context_vec_2\",context_vec_2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QxtdHsfvzC_-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55ea0cee-3de1-4511-ae92-d6f7429d16d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.4306, 1.4551])\n",
            "query.shape: torch.Size([6, 2])\n",
            "keys.shape: torch.Size([6, 2])\n",
            "values.shape: torch.Size([6, 2])\n",
            "tensor(1.8524)\n",
            "attn_scores_2 tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n",
            "attn_weights_2 tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])\n",
            "context_vec_2 tensor([0.3061, 0.8210])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Listing 3.1 A compact self-attention class\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "class SelfAttention_v1(nn.Module):\n",
        "\n",
        "    def __init__(self, d_in, d_out):\n",
        "\n",
        "        super().__init__()\n",
        "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
        "        self.W_key = nn.Parameter(torch.rand(d_in, d_out))\n",
        "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "            keys = x @ self.W_key\n",
        "            queries = x @ self.W_query\n",
        "            values = x @ self.W_value\n",
        "            attn_scores = queries @ keys.T # omega\n",
        "            attn_weights = torch.softmax(\n",
        "            attn_scores / keys.shape[-1]**0.5, dim=-1 )\n",
        "            context_vec = attn_weights @ values\n",
        "            return context_vec\n",
        "\n",
        "torch.manual_seed(123)\n",
        "sa_v1 = SelfAttention_v1(d_in, d_out)\n",
        "print(sa_v1(inputs))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q8ItnaLon9k5",
        "outputId": "81e1b048-7b08-4fdd-af44-75b3d972fa66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.2996, 0.8053],\n",
            "        [0.3061, 0.8210],\n",
            "        [0.3058, 0.8203],\n",
            "        [0.2948, 0.7939],\n",
            "        [0.2927, 0.7891],\n",
            "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "We can improve the implementation further by utilizing SelfAttention_v1 PyTorch’s\n",
        "layers, which effectively perform matrix multiplication when nn.Linear the bias units are disabled.\n",
        " Additionally, a significant advantage of using nn.Linear \"\"\"\n",
        "# Listing 3.2 A self-attention class using PyTorch’s Linear layers\n",
        "\n",
        "class SelfAttention_v2(nn.Module):\n",
        "\n",
        "      def __init__(self, d_in, d_out, qkv_bias=False):\n",
        "\n",
        "            super().__init__()\n",
        "            self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "            self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "            self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "\n",
        "      def forward(self, x):\n",
        "\n",
        "            keys = self.W_key(x)\n",
        "            queries = self.W_query(x)\n",
        "            values = self.W_value(x)\n",
        "            attn_scores = queries @ keys.T\n",
        "            attn_weights = torch.softmax(\n",
        "            attn_scores / keys.shape[-1]**0.5, dim=-1\n",
        "            )\n",
        "            context_vec = attn_weights @ values\n",
        "            return context_vec\n",
        "\n",
        "# You can use the SelfAttention_v2 similar to SelfAttention_v1:\n",
        "torch.manual_seed(789)\n",
        "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
        "print(sa_v2(inputs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVHR5xHhqw5f",
        "outputId": "45010521-9b90-4a49-9253-23d95fc87b48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.0739,  0.0713],\n",
            "        [-0.0748,  0.0703],\n",
            "        [-0.0749,  0.0702],\n",
            "        [-0.0760,  0.0685],\n",
            "        [-0.0763,  0.0679],\n",
            "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Listing 3.4 A wrapper class to implement multi-head attention\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "\n",
        "class CausalAttention(nn.Module):\n",
        "      def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n",
        "\n",
        "          super().__init__()\n",
        "          self.d_out = d_out\n",
        "          self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "          self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "          self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "          self.dropout = nn.Dropout(dropout)\n",
        "          self.register_buffer(\n",
        "          'mask',\n",
        "          torch.triu(torch.ones(context_length, context_length),\n",
        "          diagonal=1)\n",
        "          )\n",
        "\n",
        "      def forward(self, x):\n",
        "          b, num_tokens, d_in = x.shape\n",
        "          keys = self.W_key(x)\n",
        "          queries = self.W_query(x)\n",
        "          values = self.W_value(x)\n",
        "          attn_scores = queries @ keys.transpose(1, 2)\n",
        "          attn_scores.masked_fill_(\n",
        "          self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n",
        "          attn_weights = torch.softmax(\n",
        "          attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "          attn_weights = self.dropout(attn_weights)\n",
        "          context_vec = attn_weights @ values\n",
        "          return context_vec\n",
        "\n",
        "torch.manual_seed(123)\n",
        "batch = torch.randn(2, 4, 3)\n",
        "context_length = batch.shape[1]\n",
        "d_in, d_out = 3, 2\n",
        "ca = CausalAttention(d_in, d_out, context_length, 0.0)\n",
        "context_vecs = ca(batch)\n",
        "print(\"context_vecs.shape:\", context_vecs.shape)\n",
        "\n",
        "print(\"-\" * 60)\n",
        "\n",
        "class MultiHeadAttentionWrapper(nn.Module):\n",
        "\n",
        "       def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "\n",
        "            super().__init__()\n",
        "            self.heads = nn.ModuleList([CausalAttention(d_in, d_out, context_length, dropout, qkv_bias)\n",
        "\n",
        "                 for _ in range(num_heads)])\n",
        "\n",
        "       def forward(self, x):\n",
        "\n",
        "                return torch.cat([head(x) for head in self.heads], dim=-1)\n",
        "\n",
        "torch.manual_seed(123)\n",
        "context_length = batch.shape[1]\n",
        "d_in, d_out = 3, 2\n",
        "mha = MultiHeadAttentionWrapper(d_in, d_out, context_length, 0.1, 2)\n",
        "context_vecs = mha(batch)\n",
        "print(context_vecs)\n",
        "print(\"context_vecs.shape:\", context_vecs.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFOdkpeDifgC",
        "outputId": "1828cb36-d22c-4485-f1a8-1240fe1c9417"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "context_vecs.shape: torch.Size([2, 4, 2])\n",
            "------------------------------------------------------------\n",
            "tensor([[[-4.2997e-02, -8.3028e-02,  4.5441e-02,  2.0001e-02],\n",
            "         [-6.8821e-03,  4.9498e-02,  1.0061e-02, -1.0784e-03],\n",
            "         [ 9.8406e-02,  8.0856e-02, -9.0154e-02, -8.4863e-02],\n",
            "         [ 2.1659e-01, -3.7558e-01, -5.0653e-02,  3.7103e-02]],\n",
            "\n",
            "        [[-1.2866e+00,  4.2165e-01,  1.3673e+00,  2.9141e-01],\n",
            "         [ 1.1901e+00,  4.3084e-02,  1.7666e-01, -1.2782e-01],\n",
            "         [-1.7002e-01,  1.8293e-01, -5.5676e-01, -2.3773e-01],\n",
            "         [ 1.0950e-01, -2.4061e-01,  1.4731e-01,  2.3350e-01]]],\n",
            "       grad_fn=<CatBackward0>)\n",
            "context_vecs.shape: torch.Size([2, 4, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3.6.2 Implementing multi-head attention with weight splits\n",
        "\n",
        "import torch\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "     def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "\n",
        "        super().__init__()\n",
        "        assert (d_out % num_heads == 0), \\\n",
        "        \"d_out must be divisible by num_heads\"\n",
        "        self.d_out = d_out\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_out // num_heads\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.out_proj = nn.Linear(d_out, d_out)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer(\n",
        "        \"mask\",\n",
        "        torch.triu(torch.ones(context_length, context_length),\n",
        "        diagonal=1)\n",
        "        )\n",
        "\n",
        "     def forward(self, x):\n",
        "\n",
        "            b, num_tokens, d_in = x.shape\n",
        "            keys = self.W_key(x)\n",
        "            queries = self.W_query(x)\n",
        "            values = self.W_value(x)\n",
        "\n",
        "            keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "            values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "            queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "\n",
        "            \"\"\" We implicitly split the matrix by adding a num_heads dimension. Then we unroll the last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim).\"\"\"\n",
        "            keys = keys.transpose(1, 2)\n",
        "            queries = queries.transpose(1, 2) # Transposes from shape (b, num_tokens, num_heads, head_dim) to (b, num_heads,num_tokens, head_dim)\n",
        "            values = values.transpose(1, 2)\n",
        "\n",
        "            attn_scores = queries @ keys.transpose(2, 3)\n",
        "            mask_bool = self.mask.bool()[:num_tokens, :num_tokens]  # Masks truncated to the number of tokens\n",
        "            attn_scores.masked_fill_(mask_bool, -torch.inf)  # Uses the mask to fill attention scores\n",
        "            attn_weights = torch.softmax(\n",
        "            attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "            attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "\n",
        "            context_vec = (attn_weights @ values).transpose(1, 2) # Tensor shape:(b, num_tokens,n_heads,head_dim)\n",
        "            context_vec = context_vec.contiguous().view( b, num_tokens, self.d_out ) # Combines heads, where self.d_out= self.num_heads * self.head_dim\n",
        "            context_vec = self.out_proj(context_vec) # Adds an optional linear projection\n",
        "            return context_vec\n",
        "\n",
        "\n",
        "torch.manual_seed(123)\n",
        "batch_size, context_length, d_in = batch.shape\n",
        "d_out = 2\n",
        "mha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=2)\n",
        "batch = torch.randn(2, 4, 2)\n",
        "context_vecs = mha(batch)\n",
        "print(context_vecs)\n",
        "print(\"context_vecs.shape:\", context_vecs.shape)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "reFwW5oSpNgH",
        "outputId": "fc6e6c04-27ef-41fb-aeb1-7827fc85363b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "mat1 and mat2 shapes cannot be multiplied (8x2 and 3x2)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-a49d7b5d1275>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0mmha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultiHeadAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m \u001b[0mcontext_vecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext_vecs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"context_vecs.shape:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_vecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-a49d7b5d1275>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0mkeys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m             \u001b[0mqueries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (8x2 and 3x2)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Implementing a GPT model from scratch to generate text \"\"\"\n",
        "\n",
        "# We specify the configuration of the small GPT-2 model via the following Python dictionary, which we will use in the code examples later\n",
        "\n",
        "GPT_CONFIG_124M = {\n",
        "\n",
        "    \"vocab_size\": 50257, # Vocabulary size\n",
        "    \"context_length\": 1024, # Context length\n",
        "    \"emb_dim\": 768, # Embedding dimension\n",
        "    \"n_heads\": 12, # Number of attention heads\n",
        "    \"n_layers\": 12, # Number of layers\n",
        "    \"drop_rate\": 0.1, # Dropout rate\n",
        "    \"qkv_bias\": False # Query-Key-Value bias\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "# Listing 4.1 A placeholder GPT model architecture class\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "class DummyGPTModel(nn.Module):\n",
        "\n",
        "      def __init__(self, cfg):\n",
        "\n",
        "            super().__init__()\n",
        "            self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
        "            self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
        "            self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
        "            self.trf_blocks = nn.Sequential(\n",
        "            *[DummyTransformerBlock(cfg)\n",
        "            for _ in range(cfg[\"n_layers\"])])\n",
        "            self.final_norm = DummyLayerNorm(cfg[\"emb_dim\"])\n",
        "            self.out_head = nn.Linear(\n",
        "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
        "\n",
        "      def forward(self, in_idx):\n",
        "\n",
        "          batch_size, seq_len = in_idx.shape\n",
        "          tok_embeds = self.tok_emb(in_idx)\n",
        "          pos_embeds = self.pos_emb(\n",
        "          torch.arange(seq_len, device=in_idx.device))\n",
        "          x = tok_embeds + pos_embeds\n",
        "          x = self.drop_emb(x)\n",
        "          x = self.trf_blocks(x)\n",
        "          x = self.final_norm(x)\n",
        "          logits = self.out_head(x)\n",
        "          return logits"
      ],
      "metadata": {
        "id": "sxDv4ldQmO9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Listing 4.5 A neural network to illustrate shortcut connections\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# We specify the configuration of the small GPT-2 model via the following Python dictionary, which we will use in the code examples later\n",
        "\n",
        "GPT_CONFIG_124M = {\n",
        "\n",
        "    \"vocab_size\": 50257, # Vocabulary size\n",
        "    \"context_length\": 1024, # Context length\n",
        "    \"emb_dim\": 768, # Embedding dimension\n",
        "    \"n_heads\": 12, # Number of attention heads\n",
        "    \"n_layers\": 12, # Number of layers\n",
        "    \"drop_rate\": 0.1, # Dropout rate\n",
        "    \"qkv_bias\": False # Query-Key-Value bias\n",
        "\n",
        "}\n",
        "\n",
        "# Listing 4.3 An implementation of the GELU activation function\n",
        "\n",
        "class GELU(nn.Module):\n",
        "\n",
        "      def __init__(self):\n",
        "           super().__init__()\n",
        "\n",
        "      def forward(self, x):\n",
        "\n",
        "            return 0.5 * x * (1 + torch.tanh(\n",
        "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
        "            (x + 0.044715 * torch.pow(x, 3))\n",
        "            ))\n",
        "\n",
        "# Listing 4.4 A feed forward neural network module\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "\n",
        "      def __init__(self, cfg):\n",
        "\n",
        "            super().__init__()\n",
        "            self.layers = nn.Sequential(\n",
        "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
        "            GELU(),\n",
        "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
        "            )\n",
        "\n",
        "      def forward(self, x):\n",
        "\n",
        "          return self.layers(x)\n",
        "\n",
        "ffn = FeedForward(GPT_CONFIG_124M)\n",
        "x = torch.rand(2, 3, 768)\n",
        "out = ffn(x)\n",
        "print(out.shape)\n",
        "\n",
        "gelu, relu = GELU(), nn.ReLU()\n",
        "x = torch.linspace(-3, 3, 100)\n",
        "y_gelu, y_relu = gelu(x), relu(x)\n",
        "\n",
        "plt.figure(figsize=(8, 3))\n",
        "\n",
        "for i, (y, label) in enumerate(zip([y_gelu, y_relu], [\"GELU\", \"ReLU\"]), 1):\n",
        "\n",
        "      plt.subplot(1, 2, i)\n",
        "      plt.plot(x, y)\n",
        "      plt.title(f\"{label} activation function\")\n",
        "      plt.xlabel(\"x\")\n",
        "      plt.ylabel(f\"{label}(x)\")\n",
        "      plt.grid(True)\n",
        "      plt.tight_layout()\n",
        "      plt.show()\n",
        "\n",
        "class ExampleDeepNueralNetwork(nn.Module):\n",
        "\n",
        "      def __init__ (self, layer_sizes, use_shortcut):\n",
        "\n",
        "           super().__init__()\n",
        "           self.use_shortcut = use_shortcut\n",
        "           self.layers = nn.ModuleList([\n",
        "\n",
        "                nn.Sequential(nn.Linear(layer_sizes[0], layer_sizes[1]),\n",
        "                GELU()),\n",
        "                nn.Sequential(nn.Linear(layer_sizes[1], layer_sizes[2]),\n",
        "                GELU()),\n",
        "                nn.Sequential(nn.Linear(layer_sizes[2], layer_sizes[3]),\n",
        "                GELU()),\n",
        "                nn.Sequential(nn.Linear(layer_sizes[3], layer_sizes[4]),\n",
        "                GELU()),\n",
        "                nn.Sequential(nn.Linear(layer_sizes[4], layer_sizes[5]),\n",
        "                GELU())\n",
        "                ])\n",
        "\n",
        "      def forward(self, x):\n",
        "\n",
        "         for layer in self.layers:\n",
        "\n",
        "             layer_out = layer(x)\n",
        "\n",
        "             if self.use_shortcut and x.shape == layer_out.shape: # useing shortcut funcation to added input of L1 to output L1 and so on\n",
        "                 x = x + layer_out\n",
        "             else:\n",
        "                 x = layer_out\n",
        "         return x\n",
        "\n",
        "layer_sizes = [3, 3, 3, 3, 3, 1]\n",
        "\n",
        "sample_input = torch.tensor([[1., 0., -1.]])\n",
        "\n",
        "torch.manual_seed(123)\n",
        "model_without_shortcut = ExampleDeepNueralNetwork(layer_sizes, use_shortcut=False)\n",
        "\n",
        "\n",
        "# Next, we implement a function that computes the gradients in the model’s backward pass\n",
        "\n",
        "def print_gradients(model, x):\n",
        "\n",
        "    output = model(x) # forward Pass\n",
        "    target = torch.tensor([[0.]])\n",
        "\n",
        "    loss = nn.MSELoss() # calculate the Loss based on how to close the target and output are\n",
        "    loss = loss(output, target)\n",
        "\n",
        "    loss.backward() # bachword pass to calculate the the gradients\n",
        "\n",
        "\n",
        "    for name, param in model.named_parameters():\n",
        "\n",
        "        if 'weight' in name:\n",
        "\n",
        "            print(f\"{name} has gradient mean of {param.grad.abs().mean().item()}\")\n",
        "\n",
        "print(\"-\" * 70)\n",
        "\n",
        "\"\"\" The output of the function shows, the gradients become smaller print_gradients\n",
        "as we progress from the last layer .4) to the first layer .0), which is layers layers\n",
        "a phenomenon called the vanishing gradient problem.\"\"\"\n",
        "\n",
        "print_gradients(model_without_shortcut, sample_input)\n",
        "\n",
        "print(\"-\" * 70)\n",
        "\n",
        "# Let’s now instantiate a model with skip connections and see how it compares:\n",
        "\"\"\" The last layer.4) still has a larger gradient than the other layers. However,\n",
        "(layers the gradient value stabilizes as we progress toward the first layer .0)\n",
        " and layers doesn’t shrink to a \"vanishingly small value\". \"\"\"\n",
        "\n",
        "torch.manual_seed(123)\n",
        "model_with_shortcut = ExampleDeepNueralNetwork(layer_sizes, use_shortcut=True)\n",
        "print_gradients(model_with_shortcut, sample_input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "k3nqbQvJyj17",
        "outputId": "8e0176f8-79bd-4cfe-befb-fef08d994461"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3, 768])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x300 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------------\n",
            "layers.0.0.weight has gradient mean of 0.00020173584925942123\n",
            "layers.1.0.weight has gradient mean of 0.00012011159560643137\n",
            "layers.2.0.weight has gradient mean of 0.0007152040489017963\n",
            "layers.3.0.weight has gradient mean of 0.0013988736318424344\n",
            "layers.4.0.weight has gradient mean of 0.005049645435065031\n",
            "----------------------------------------------------------------------\n",
            "layers.0.0.weight has gradient mean of 0.22169791162014008\n",
            "layers.1.0.weight has gradient mean of 0.20694105327129364\n",
            "layers.2.0.weight has gradient mean of 0.32896995544433594\n",
            "layers.3.0.weight has gradient mean of 0.2665732204914093\n",
            "layers.4.0.weight has gradient mean of 1.3258540630340576\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Hello, do you like tea?\"\n",
        "print(tokenizer.encode(text))\n",
        "\n",
        "all_tokens = sorted(list(set(preprocced)))\n",
        "all_tokens.extend(\"<|endoftext|>\",\"<|unk|>\")\n",
        "\n",
        "vocab = {token:integer for integer, token in enumerate(all_tokens)}\n",
        "print(len(vocab.items()))\n",
        "for i, item in enumerate(list(vocab.items())[-5:]):\n",
        " print(item)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "MK5u0yrvNpHZ",
        "outputId": "d94ac574-c3b4-4337-8186-1d28d5b0c405"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'Hello'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-253f11091681>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Hello, do you like tea?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mall_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocced\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mall_tokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"<|endoftext|>\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"<|unk|>\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-27-2d70dce9494d>\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     15\u001b[0m       preprocessed = [\n\u001b[1;32m     16\u001b[0m                 item.strip() for item in preprocessed if item.strip()]\n\u001b[0;32m---> 17\u001b[0;31m       \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr_into_int\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreprocced\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-27-2d70dce9494d>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     15\u001b[0m       preprocessed = [\n\u001b[1;32m     16\u001b[0m                 item.strip() for item in preprocessed if item.strip()]\n\u001b[0;32m---> 17\u001b[0;31m       \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr_into_int\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreprocced\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Hello'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_Y1g0ydGDhy",
        "outputId": "5f782811-6efb-4acb-9729-a16631acf6ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ',', '', ' ', 'world', '.', '', ' ', 'This', ',', '', ' ', 'is', ' ', 'a', ' ', 'test', '.', '']\n",
            "['Hello', ',', 'world', '.', 'This', ',', 'is', 'a', 'test', '.']\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "text = \"Hello, world. This, is a test.\"\n",
        "result = re.split(r'([,.]|\\s)', text)\n",
        "print(result)\n",
        "result = [item for item in result if item.strip()]\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "-JEqhnmyKgPq"
      }
    }
  ]
}